{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dogs_vs_Cat.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXHA7Vo12omZ"
      },
      "source": [
        "import csv\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import transforms,models,datasets\n",
        "from torch import optim\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uprFyVv6E8-"
      },
      "source": [
        "!mkdir reduced_data\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/reduced_data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/reduced_data\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSOc9yB32y8E"
      },
      "source": [
        "batch_size = 1024\n",
        "\n",
        "train_transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(\"/content/reduced_data\", transform= train_transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size ,shuffle=True)\n",
        "\n",
        "test_transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(\"/content/reduced_data\", transform= test_transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size ,shuffle=True)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iun5FWzN6ijl"
      },
      "source": [
        "model = models.densenet121(pretrained = True)\n",
        "for params in model.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "    ('fc1',nn.Linear(1024,500)),\n",
        "    ('relu',nn.ReLU()),\n",
        "    ('fc2',nn.Linear(500,2)),\n",
        "    ('Output',nn.LogSoftmax(dim=1))\n",
        "]))\n",
        "\n",
        "model.classifier = classifier\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUkI4fNY-Itb",
        "outputId": "423ea950-f0f6-4533-9b65-b628c12a0136"
      },
      "source": [
        "optimizer = optim.Adam(model.classifier.parameters())\n",
        "criterian = nn.NLLLoss()\n",
        "list_train_loss = []\n",
        "list_test_loss = []\n",
        "f = open('metrics.csv', 'w')\n",
        "file = csv.writer(f)\n",
        "file.writerow(['Epoch', 'Train loss', 'Test loss', 'Accuracy'])\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    test_loss = 0\n",
        "    for bat, (img, label) in enumerate(train_loader):\n",
        "        # moving batch and lables to gpu\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(img)\n",
        "        loss = criterian(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss = train_loss + loss.item()\n",
        "        # print(bat)\n",
        "\n",
        "    accuracy = 0\n",
        "    for bat, (img, label) in enumerate(test_loader):\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        logps = model(img)\n",
        "        loss = criterian(logps, label)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        ps = torch.exp(logps)\n",
        "        top_ps, top_class = ps.topk(1, dim=1)\n",
        "        equality = top_class == label.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equality.type(torch.FloatTensor)).item()\n",
        "\n",
        "    list_train_loss.append(train_loss / 20)\n",
        "    list_test_loss.append(test_loss / 20)\n",
        "    print('epoch: ', epoch, '    train_loss:  ', train_loss / 20, '   test_loss:    ', test_loss / 20,\n",
        "          '    accuracy:  ', accuracy / len(test_loader))\n",
        "    file.writerow([epoch,train_loss / 20, test_loss / 20,accuracy / len(test_loader)])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0     train_loss:   0.05865321457386017    test_loss:     0.0388381689786911     accuracy:   0.823913961648941\n",
            "epoch:  1     train_loss:   0.028877216577529907    test_loss:     0.015733447670936585     accuracy:   0.974007785320282\n",
            "epoch:  2     train_loss:   0.015653782337903977    test_loss:     0.011155467480421066     accuracy:   0.9686868488788605\n",
            "epoch:  3     train_loss:   0.009800592064857483    test_loss:     0.012892067059874534     accuracy:   0.9468384981155396\n",
            "epoch:  4     train_loss:   0.007426820322871208    test_loss:     0.007265613973140716     accuracy:   0.9763489365577698\n",
            "epoch:  5     train_loss:   0.006167935580015183    test_loss:     0.00622833389788866     accuracy:   0.979704737663269\n",
            "epoch:  6     train_loss:   0.005749544501304627    test_loss:     0.00649643074721098     accuracy:   0.9785276651382446\n",
            "epoch:  7     train_loss:   0.004900055937469006    test_loss:     0.006786392070353031     accuracy:   0.9733440577983856\n",
            "epoch:  8     train_loss:   0.004684225469827652    test_loss:     0.005480760149657727     accuracy:   0.9824840426445007\n",
            "epoch:  9     train_loss:   0.004314455576241017    test_loss:     0.00468551404774189     accuracy:   0.9859521389007568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM789jXIAO8s"
      },
      "source": [
        "# torch.save(model.state_dict(), 'model.pth')\n",
        "torch.save(model, \"model_pytorch.h5\")\n",
        "f.close()"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}